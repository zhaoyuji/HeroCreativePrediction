{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import *\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34422060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUDIO_OUTPUT_PATH = 'processed_video/'\n",
    "\n",
    "MUSIC_FEATURE_BEFORE_CLUSTER = 'processed_feature/music.csv'\n",
    "MUSIC_FEATURE = 'processed_feature/music_with_cluster.csv'\n",
    "\n",
    "KMEANS_MODEL = \"cluster_model/kmeans_model.pkl\"\n",
    "YAMNET_MODEL = '/Users/yujizhao/Projects/analysis/video/pretrained_models/yamnet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d6245",
   "metadata": {},
   "source": [
    "## Handcrafted Audio Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-Crossing Rate，ZCR\n",
    "# 是指一个信号的符号变化的比率，例如信号从正数变成负数或反向。具体来说，单位时间内信号通过零值的次数就称为过零率。\n",
    "# 这个特征在电磁学领域得到广泛使用，常用于分析一段电流。对于连续语音信号，过零率可以考查其时域波形通过时间轴的情况。\n",
    "# 对于离散信号，实质上就是信号采样点符号变化的次数。在一定程度上，过零率可以反映出频率的信息，比如正弦信号的平均过零率就是信号的频率除以两倍采样频率。\n",
    "# 浊音平均过零率低,集中在低频端; 轻音过零率高,集中在高频端 \n",
    "\n",
    "# Zero-Crossing Rate (ZCR): The rate at which the audio signal changes sign. It's useful for detecting the presence of noise or percussive sounds.\n",
    "# Amplitude Envelope: The smooth curve outlining the extremes of the audio waveform. It can represent the dynamics of the signal.\n",
    "# envelope = librosa.onset.onset_strength(y)\n",
    "# Energy: The sum of squares of the signal amplitude, representing the loudness of the audio.\n",
    "\n",
    "\n",
    "def detect_sounds(audio_file, silence_thresh=-40, min_silence_len=500):\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    # Detect non-silent periods\n",
    "    nonsilent_ranges = detect_nonsilent(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "    return audio, nonsilent_ranges\n",
    "\n",
    "def calculate_non_silent_duration(nonsilent_ranges):\n",
    "    # Calculate the total duration of non-silent periods in milliseconds\n",
    "    total_non_silent_duration = sum(end_ms - start_ms for start_ms, end_ms in nonsilent_ranges)\n",
    "    return total_non_silent_duration / 1000  \n",
    "\n",
    "\n",
    "def sqrt_energy(x):\n",
    "    squared_sum = np.square(x).sum()\n",
    "    energy = squared_sum / len(x)\n",
    "    rms_energy = np.sqrt(energy)\n",
    "    return rms_energy\n",
    "\n",
    "def create_folder(directory):    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "# Define frequency bands (in Hz)\n",
    "bands = {\n",
    "    'sub-bass': (20, 60),\n",
    "    'bass': (60, 250),\n",
    "    'low-midrange': (250, 500),\n",
    "    'midrange': (500, 2000),\n",
    "    'upper-midrange': (2000, 4000),\n",
    "    'presence': (4000, 6000),\n",
    "    'brilliance': (6000, 20000)\n",
    "}\n",
    "\n",
    "# Convert frequency bands to bins\n",
    "def freq_to_bin(frequency, sr, n_fft):\n",
    "    return int(frequency * n_fft / sr)\n",
    "\n",
    "def classify_bpm_detailed(bpm):\n",
    "    if bpm <= 40:\n",
    "        return 'grave' # 'Grave (very slow)'\n",
    "    elif bpm <= 60:\n",
    "        return 'largo'# 'Largo (broadly)'\n",
    "    elif bpm <= 66:\n",
    "        return  'larghetto' # 'Larghetto (rather broadly)'\n",
    "    elif bpm <= 76:\n",
    "        return 'adagio' # 'Adagio (slow and stately)'\n",
    "    elif bpm <= 108:\n",
    "        return 'andante' # 'Andante (at a walking pace)'\n",
    "    elif bpm <= 120:\n",
    "        return 'moderato' # 'Moderato (moderately)'\n",
    "    elif bpm <= 156:\n",
    "        return 'allegro' # 'Allegro (fast, quickly and bright)'\n",
    "    elif bpm <= 176:\n",
    "        return 'vivace' # 'Vivace (lively and fast)'\n",
    "    else:\n",
    "        return 'presto' # 'Presto (extremely fast)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "music_features = pd.DataFrame()\n",
    "n_fft = 2048\n",
    "# separator = Separator('spleeter:2stems') ## Separator does not work properly so not used in final features \n",
    "for x in tqdm(final_dataset[idx].values):\n",
    "    file_path = video_path+'/'+x+'.mp4'\n",
    "    video = VideoFileClip(file_path)\n",
    "    output_directory = AUDIO_OUTPUT_PATH + x \n",
    "    create_folder(output_directory)\n",
    "    cur_dict = {'creative_code': x} \n",
    "    audio_file = output_directory + '/origin_audio.wav'\n",
    "\n",
    "    if not os.path.exists(audio_file):\n",
    "        audio = video.audio \n",
    "        audio.write_audiofile(audio_file, logger=None)\n",
    "\n",
    "    #     # Separate the audio file to accompaniment and vocals \n",
    "    #     # separator.separate_to_file(audio_file, output_directory)\n",
    "    #     separate_to_file_with_timeout(separator, audio_file, output_directory, timeout_seconds)\n",
    "\n",
    "    wave_data, sr = librosa.load(audio_file)\n",
    "    \n",
    "    cur_dict['zcr'] = sum(librosa.zero_crossings(y=wave_data, pad=False)) / len(wave_data)  \n",
    "    rms = librosa.feature.rms(y=wave_data)[0]\n",
    "    cur_dict['rms_average'] = np.sum(rms) / len(rms)  # 求全部帧的短时能量均值\n",
    "    cur_dict['rms_diff'] = np.max(rms) - np.min(rms)\n",
    "    cur_dict['rms_std'] = np.std(rms)  \n",
    "    cur_dict['rms_high'] = np.quantile(rms, 0.75)  \n",
    "    cur_dict['rms_low'] = np.quantile(rms, 0.25) \n",
    "    cur_dict['energy'] = sqrt_energy(rms)\n",
    "\n",
    "    stft = np.abs(librosa.stft(wave_data))\n",
    "    freq_bins = {band: (freq_to_bin(low, sr, n_fft), freq_to_bin(high, sr, n_fft)) for band, (low, high) in bands.items()}\n",
    "\n",
    "    # Compute the energy in each frequency band\n",
    "    band_energies = {}\n",
    "    for band, (low_bin, high_bin) in freq_bins.items():\n",
    "        band_energies[band] = np.sum(stft[low_bin:high_bin, :])\n",
    "    # Normalize by the total energy\n",
    "    total_energy = np.sum(stft)\n",
    "    normalized_band_energies = {band: energy / total_energy for band, energy in band_energies.items()}\n",
    "    cur_dict['frequency_bands_sub_bass'] = normalized_band_energies['sub-bass']\n",
    "    cur_dict['frequency_bands_bass'] = normalized_band_energies['bass']\n",
    "    cur_dict['frequency_bands_low_midrange'] = normalized_band_energies['low-midrange']\n",
    "    cur_dict['frequency_bands_midrange'] = normalized_band_energies['midrange']\n",
    "    cur_dict['frequency_bands_upper_midrange'] = normalized_band_energies['upper-midrange']\n",
    "    cur_dict['frequency_bands_presence'] = normalized_band_energies['presence']\n",
    "    cur_dict['frequency_bands_brilliance'] = normalized_band_energies['brilliance']\n",
    "\n",
    "    n_mfcc = 100\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=wave_data, sr=sr, n_mfcc=n_mfcc).T,axis=0)\n",
    "    np.save(os.path.join(AUDIO_OUTPUT_PATH, x)+'/audio_mfcc100.npy', mfccs)\n",
    "\n",
    "    # tempo \n",
    "    cur_dict['overall_tempo'] = librosa.feature.tempo(y=wave_data, sr=sr)[0]\n",
    "    \n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=wave_data, sr=sr)\n",
    "    beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "    bpm = 60 / np.diff(beat_times)  # Calculate BPM from inter-beat intervals\n",
    "    bpm_categories = [classify_bpm_detailed(b) for b in bpm]\n",
    "    bpm_distribution = Counter(bpm_categories)\n",
    "    total_bpm = sum(bpm_distribution.values())\n",
    "    bpm_ratio = {category: count / total_bpm for category, count in bpm_distribution.items()}\n",
    "    for cat in ['grave', 'largo', 'larghetto','adagio','andante','moderato','allegro','vivace','presto']: \n",
    "        cur_dict['beats_'+cat] = bpm_ratio.get(cat, 0.0)\n",
    "    \n",
    "    cur_df = pd.DataFrame(cur_dict, index=[0])\n",
    "    music_features = pd.concat([music_features, cur_df], axis=0)\n",
    "    if len(music_features) % 10 == 0:\n",
    "        music_features.to_csv(MUSIC_FEATURE_BEFORE_CLUSTER,index=False) \n",
    "music_features.to_csv(MUSIC_FEATURE_BEFORE_CLUSTER,index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1973bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_features.to_csv(MUSIC_FEATURE_BEFORE_CLUSTER,index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096956ec",
   "metadata": {},
   "source": [
    "## Audio Cluster Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17716c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav\n",
    "\n",
    "# Function to extract YAMNet embeddings from audio file\n",
    "def extract_yamnet_embeddings(model, audio_file):\n",
    "    wav_data = load_wav_16k_mono(audio_file)\n",
    "    scores, embeddings, spectrogram = model(wav_data)\n",
    "    return tf.expand_dims(embeddings, axis=-1).numpy()\n",
    "\n",
    "# Function to compute average embedding\n",
    "def compute_avg_embedding(embeddings):\n",
    "    avg_embedding = tf.reduce_mean(embeddings, axis=0, keepdims=True)\n",
    "    return np.reshape(avg_embedding, -1)\n",
    "\n",
    "# Function to perform K-means clustering on embeddings\n",
    "def cluster_audio_segments(embeddings, num_clusters, joblib_file):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    cluster_model = kmeans.fit(embeddings)\n",
    "        \n",
    "    # Save the model to a file\n",
    "    joblib.dump(cluster_model, joblib_file)\n",
    "    print(f\"Model saved to {joblib_file}\")\n",
    "    \n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# Function to visualize clusters using PCA\n",
    "def visualize_clusters(embeddings, clusters, num_clusters, video_ids=None):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for cluster_id in range(num_clusters):\n",
    "        cluster_points = reduced_embeddings[clusters == cluster_id]\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_id}')\n",
    "        if video_ids is not None:\n",
    "            for i, point in enumerate(cluster_points):\n",
    "                plt.annotate(video_ids[i], (point[0], point[1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.title('YAMNet Audio Clustering')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    # Load the model from the file\n",
    "    loaded_kmeans = joblib.load(KMEANS_MODEL)\n",
    "    print(f\"Model loaded from {KMEANS_MODEL}\")\n",
    "except:\n",
    "    ## RUN K MEANS \n",
    "    \n",
    "    # Load YAMNet model from TensorFlow Hub\n",
    "    # YAMNET_URL = \"https://tfhub.dev/google/yamnet/1\"\n",
    "    model = hub.load(YAMNET_MODEL)\n",
    "    audio_dirs = os.listdir(AUDIO_OUTPUT_PATH)  \n",
    "\n",
    "    # Extract YAMNet embeddings for each audio file\n",
    "    embeddings_list = []\n",
    "    video_ids_list = []\n",
    "\n",
    "    for x in audio_dirs:\n",
    "        if x == '.DS_Store': continue\n",
    "        audio_file = AUDIO_OUTPUT_PATH + os.path.join(x, 'origin_audio.wav')  \n",
    "        embeddings = extract_yamnet_embeddings(model, audio_file)\n",
    "        embeddings_list.append(compute_avg_embedding(embeddings))\n",
    "        video_ids_list.append(x)\n",
    "\n",
    "    all_embeddings = np.vstack(embeddings_list)\n",
    "    video_ids = np.array(video_ids_list)\n",
    "\n",
    "    print(all_embeddings.shape)\n",
    "    unique_rows, unique_indices = np.unique(all_embeddings, axis=0, return_index=True)\n",
    "    unique_video_ids = video_ids[unique_indices]\n",
    "    print(unique_rows.shape)\n",
    "\n",
    "    # Generate synthetic dataset\n",
    "    X = unique_rows \n",
    "    MAX_K = 20\n",
    "    # Elbow Method\n",
    "    wcss = []\n",
    "    for k in range(1, MAX_K+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    plt.plot(range(1, MAX_K+1), wcss, 'bx-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Silhouette Method\n",
    "    silhouette_scores = []\n",
    "    for k in range(2, MAX_K+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        score = silhouette_score(X, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "    plt.plot(range(2, MAX_K+1), silhouette_scores, 'bx-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Method for Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    BEST_K = 6\n",
    "\n",
    "    clusters = cluster_audio_segments(unique_rows, BEST_K, KMEANS_MODEL)\n",
    "    # Visualize clusters\n",
    "    visualize_clusters(unique_rows, clusters, BEST_K, unique_video_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dee69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(YAMNET_MODEL)\n",
    "cluster_feature = []\n",
    "for vid in music_features.index:\n",
    "    audio_file = AUDIO_OUTPUT_PATH + os.path.join(vid, 'origin_audio.wav')  \n",
    "    embeddings = extract_yamnet_embeddings(model, audio_file) \n",
    "    pool_embeddings = np.array(compute_avg_embedding(embeddings)).reshape(1, -1) \n",
    "    np.save(os.path.join(AUDIO_OUTPUT_PATH, vid)+'/audio_vec.npy', pool_embeddings)\n",
    "    prediction = loaded_kmeans.predict(pool_embeddings)\n",
    "    cluster_feature.append(str(prediction[0]))\n",
    "music_features_set2 = music_features.copy()\n",
    "music_features_set2['audio_cluster'] = cluster_feature\n",
    "print(music_features_set2.groupby('audio_cluster')['audio_cluster'].count())\n",
    "music_features_set2 = pd.concat([music_features, pd.get_dummies(music_features_set2[['audio_cluster']].fillna('NaN').applymap(\n",
    "                                        lambda x: str(x).lower() if isinstance(x, str) else x\n",
    "                                        ), dtype=int)], axis=1)\n",
    "music_features = music_features_set2.copy()\n",
    "music_features.to_csv(MUSIC_FEATURE, index=True) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
