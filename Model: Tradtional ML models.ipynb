{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, recall_score, precision_score, roc_curve, classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20340e3c",
   "metadata": {},
   "source": [
    "## Input \n",
    "\n",
    "The main input of this notebook is a `csv` file including processed features and labels for all videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c88e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_DATASET = 'processed_feature/total_dataset.csv'\n",
    "\n",
    "DL_TRAIN_DATASET = \"full_dataset_train.json\"\n",
    "DL_TEST_DATASET = \"full_dataset_test.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset = pickle.load(open(DL_TRAIN_DATASET,'rb'))\n",
    "testset = pickle.load(open(DL_TEST_DATASET,'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_code = trainset['train_creative_code']\n",
    "testset_code = testset['test_creative_code']\n",
    "print(len(trainset_code), len(testset_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bfa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(grid_search, X_test, y_test, threshold=None):\n",
    "    if threshold==None: \n",
    "        # Evaluate the best model on the test set\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        print(\"Classification Report for Best Model (Threshold=0.5):\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Calculate and print ROC AUC score\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"\\nROC AUC Score: {roc_auc:.2f}\")\n",
    "        \n",
    "        return best_model, y_pred_proba    \n",
    "    else: \n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report (Threshold = {threshold}):\".format(threshold=threshold))\n",
    "        print(classification_report(y_test, y_pred_threshold))\n",
    "\n",
    "        # Calculate and print ROC AUC score\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"\\nROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "def evaluate_default(predictions, test_labels, threshold=0.5):\n",
    "    predictions_binary = (predictions > threshold).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions_binary)\n",
    "    precision = precision_score(test_labels, predictions_binary)\n",
    "    recall = recall_score(test_labels, predictions_binary)\n",
    "    f1 = f1_score(test_labels, predictions_binary)\n",
    "    auc = roc_auc_score(test_labels, predictions)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "    \n",
    "    \n",
    "     # Print classification report\n",
    "    print(\"\\n\"+\"*\"*30+\"Classification Report (Threshold = {threshold}):\".format(threshold=0.5))\n",
    "    print(classification_report(test_labels, predictions_binary))\n",
    "    print(confusion_matrix(test_labels, predictions_binary.reshape(-1)))\n",
    "\n",
    "def plot_metrics_vs_threshold(y_test, predictions):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "    \n",
    "    plot_ths = []\n",
    "    recall_at_thresholds = []\n",
    "    precision_at_thresholds = []\n",
    "    f1_at_thresholds = []\n",
    "    marco_recall_thresholds = []\n",
    "    best_marco_recall = -10e100\n",
    "    best_marco_recall_threshold = 0\n",
    "    # Calculate recall, precision, and F1 score at each threshold\n",
    "    for threshold in thresholds:\n",
    "        if threshold > 1: continue \n",
    "        y_pred_threshold = (predictions > threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_threshold, average=None)\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred_threshold, average='macro')\n",
    "        recall_at_thresholds.append(recall[1])\n",
    "        precision_at_thresholds.append(precision[1])\n",
    "        f1_at_thresholds.append(f1[1])\n",
    "        marco_recall_thresholds.append(recall_macro)\n",
    "        plot_ths.append(threshold)\n",
    "        # print(round(threshold,2), recall_macro)\n",
    "        if (recall_macro > best_marco_recall) and (recall[0] > 0.7) and (recall[1] > 0.3):\n",
    "            best_marco_recall = recall_macro\n",
    "            best_marco_recall_threshold = threshold\n",
    "    \n",
    "    print(\"\\n\"+\"*\"*30+\"Evaluate metrics vs. Threshold for true class + Marco Recall\")\n",
    "        \n",
    "    # Plot the metrics\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3), sharex=True)\n",
    "\n",
    "    # Plot recall vs. threshold\n",
    "    axs[0].plot(plot_ths, recall_at_thresholds, label='Recall', color='blue')\n",
    "    axs[0].set_xlabel('Threshold')\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[0].set_ylabel('Recall')\n",
    "    axs[0].set_title('Recall vs. Threshold')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot precision vs. threshold\n",
    "    axs[1].plot(plot_ths, precision_at_thresholds, label='Precision', color='green')\n",
    "    axs[1].set_xlabel('Threshold')\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[1].set_ylabel('Precision')\n",
    "    axs[1].set_title('Precision vs. Threshold')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Plot F1 score vs. threshold\n",
    "    axs[2].plot(plot_ths, f1_at_thresholds, label='F1 Score', color='red')\n",
    "    axs[2].set_xlabel('Threshold')\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[2].set_ylabel('F1 Score')\n",
    "    axs[2].set_title('F1 Score vs. Threshold')\n",
    "    axs[2].legend()\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    # Plot F1 score vs. threshold\n",
    "    axs[3].plot(plot_ths, marco_recall_thresholds, label='Marco Recall', color='black')\n",
    "    axs[3].set_xlabel('Threshold')\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[3].set_ylabel('Marco Recall')\n",
    "    axs[3].set_title('Marco Recall vs. Threshold')\n",
    "    axs[3].legend()\n",
    "    axs[3].grid(True)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print('best recall_macro', best_marco_recall)\n",
    "    print('best_marco_recall_threshold', best_marco_recall_threshold)\n",
    "    if best_marco_recall_threshold > 0:\n",
    "        evaluate_default(predictions, y_test, best_marco_recall_threshold)\n",
    "    else: \n",
    "        print('Cannot Beat Baseline.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfccf4",
   "metadata": {},
   "source": [
    "## Ingest Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc590b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_w_music = pd.read_csv(TOTAL_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c96867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# data_sum = data_processed_w_music.groupby('labels')[\n",
    "#     ['audio_cluster_0', 'audio_cluster_1', 'audio_cluster_2', 'audio_cluster_3', 'audio_cluster_4', 'audio_cluster_5']\n",
    "# ].sum()\n",
    "\n",
    "# data_ratio = data_sum.div(data_sum.sum(axis=1), axis=0)\n",
    "\n",
    "# data_sum = data_sum.reset_index()\n",
    "# data_ratio = data_ratio.reset_index()\n",
    "# data_melted_sum = data_sum.melt(id_vars='labels', var_name='Audio Cluster', value_name='Sum')\n",
    "# data_melted_sum['Audio Cluster'] = data_melted_sum['Audio Cluster'].str.replace('audio_cluster_','')\n",
    "# data_melted_ratio = data_ratio.melt(id_vars='labels', var_name='Audio Cluster', value_name='Ratio')\n",
    "# data_melted_ratio['Audio Cluster'] = data_melted_ratio['Audio Cluster'].str.replace('audio_cluster_','')\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "# sns.barplot(x='Audio Cluster', y='Sum', hue='labels', data=data_melted_sum, ax=axes[0])\n",
    "# axes[0].set_title('Count of Different Audio Clusters Grouped by Labels')\n",
    "# axes[0].set_xlabel('Audio Cluster')\n",
    "# axes[0].set_ylabel('Sum')\n",
    "# sns.barplot(x='Audio Cluster', y='Ratio', hue='labels', data=data_melted_ratio, ax=axes[1])\n",
    "# axes[1].set_title('Ratio on Count of Audio Clusters Grouped by Labels')\n",
    "# axes[1].set_xlabel('Audio Cluster')\n",
    "# axes[1].set_ylabel('Ratio')\n",
    "# for ax in axes:\n",
    "#     ax.legend(title='Labels')\n",
    "#     # ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processed_w_music[data_processed_w_music['labels']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n'.join(data_processed_w_music.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b856cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {row['creative_code']: int(row['labels']) for i, row in data_processed_w_music[['creative_code', 'labels']].iterrows()}\n",
    "X_columns = [x for x in data_processed_w_music.columns if x not in ['creative_code', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processed_w_music.groupby(['labels', 'audio_cluster'])['labels'].count()\n",
    "# data_processed_w_music.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_train_test_split(data_processed_w_music, trainset_code, testset_code, X_columns=None):\n",
    "    if X_columns is None:\n",
    "        X_columns = [x for x in data_processed_w_music.columns if x not in ['creative_code', 'labels']]\n",
    "    Y_columns = 'labels'\n",
    "\n",
    "    X = data_processed_w_music[X_columns]\n",
    "    y = data_processed_w_music[Y_columns].astype(int)\n",
    "\n",
    "    X.index=data_processed_w_music['creative_code']\n",
    "    y.index=data_processed_w_music['creative_code']\n",
    "    # X = X.reset_index(drop=True)\n",
    "    # y = y.reset_index(drop=True)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train = X[X.index.isin(trainset_code)]\n",
    "    X_test = X[X.index.isin(testset_code)]\n",
    "    y_train = y[y.index.isin(trainset_code)].astype(int)\n",
    "    y_test = y[y.index.isin(testset_code)].astype(int)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c65620",
   "metadata": {},
   "source": [
    "## Model 0: Baseline - Domain Rule Based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f9529",
   "metadata": {},
   "source": [
    "## Model 1: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c197519",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = custom_train_test_split(data_processed_w_music, trainset_code, testset_code)\n",
    "\n",
    "imp = SimpleImputer(strategy='most_frequent')  \n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a pipeline combining feature selection and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42))),\n",
    "    ('classification', LogisticRegression(random_state=42))\n",
    "    # ('classification', LogisticRegression())\n",
    "])\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n",
    "# ‘liblinear’ and ‘newton-cholesky’ can only handle binary classification by default. To apply a one-versus-rest scheme for the multiclass setting one can wrapt it with the OneVsRestClassifier.\n",
    "\n",
    "param_grid = {\n",
    "    'feature_selection__estimator__C': [0.1, 1, 10],\n",
    "    'classification__C': [0.1, 1, 10],\n",
    "    'classification__solver': ['lbfgs', 'liblinear'],\n",
    "    'classification__class_weight': [None, 'balanced', {0: 1, 1: 5}, {0: 1, 1: 10}],\n",
    "    'classification__max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "grid_search_lr = GridSearchCV(pipeline, param_grid, scoring=scorer, cv=5, verbose=1)\n",
    "\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search_lr.best_params_}\")\n",
    "print(f\"Best AUC: {grid_search_lr.best_score_:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb984cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_lr, y_pred_lr = evaluate_best_model(grid_search_lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_default(y_pred_lr, y_test)\n",
    "\n",
    "plot_metrics_vs_threshold(y_test, y_pred_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb987b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features_mask = best_model_lr.named_steps['feature_selection'].get_support()\n",
    "coefficients = best_model_lr.named_steps['classification'].coef_[0]\n",
    "selected_feature_names = np.array(X_columns)[selected_features_mask]\n",
    "feature_importance = pd.Series(coefficients, index=selected_feature_names)\n",
    "\n",
    "feature_importance = feature_importance.sort_values(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72787af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# top_10 = feature_importance.head(10)\n",
    "# last_10 = feature_importance.tail(10)\n",
    "# combined_features = pd.concat([top_10, last_10])\n",
    "\n",
    "# # Create a bar plot using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x=combined_features.values, y=combined_features.index, palette='viridis')\n",
    "# plt.title('Top 10 and Last 10 Feature Importance for Logistic Regression')\n",
    "# plt.xlabel('Coefficient Value')\n",
    "# plt.ylabel('Feature')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaeb987",
   "metadata": {},
   "source": [
    "## Model 2: XGBOOST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = custom_train_test_split(data_processed_w_music, trainset_code, testset_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a780d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],  \n",
    "    'max_depth': [3, 6, 10],  \n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 1.0], \n",
    "    'scale_pos_weight': [1, 10, np.sum(y_train == 0) / np.sum(y_train == 1)]  # Adjust class imbalance\n",
    "}\n",
    "# 2430 fits = 84m\n",
    "\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "grid_search_xgb = GridSearchCV(classifier, param_grid, scoring=scorer, cv=5, verbose=1)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search_xgb.best_params_}\")\n",
    "print(f\"Best AUC: {grid_search_xgb.best_score_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff17e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_xgb, y_pred_xgb = evaluate_best_model(grid_search_xgb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd759c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_default(y_pred_xgb, y_test)\n",
    "plot_metrics_vs_threshold(y_test, y_pred_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importance = pd.Series(best_model_xgb.feature_importances_, index=X_columns)\n",
    "feature_importance_sorted = feature_importance.sort_values(ascending=False)[:10]\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance_sorted) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacfa6f",
   "metadata": {},
   "source": [
    "## Model 3: CATBOOST \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a817d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = custom_train_test_split(data_processed_w_music, trainset_code, testset_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'iterations': [100, 500, 1000],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'depth': [3, 6, 10],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'scale_pos_weight': [1, 5, np.sum(y_train == 0) / np.sum(y_train == 1)]  # Adjust class imbalance\n",
    "}\n",
    "# 1215 fits 223m\n",
    "\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "grid_search_cat = GridSearchCV(classifier, param_grid, scoring=scorer, cv=5, verbose=1)\n",
    "grid_search_cat.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search_cat.best_params_}\")\n",
    "print(f\"Best AUC: {grid_search_cat.best_score_:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b88c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_cat, y_pred_cat = evaluate_best_model(grid_search_cat, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f37f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_default(y_pred_cat, y_test)\n",
    "\n",
    "plot_metrics_vs_threshold(y_test, y_pred_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7003d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature_importance = pd.Series(best_model_cat.feature_importances_, index=X_columns)\n",
    "# feature_importance_sorted = feature_importance.sort_values(ascending=False)[:10]\n",
    "# print(\"Feature Importance:\")\n",
    "# print(feature_importance_sorted) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb0d83",
   "metadata": {},
   "source": [
    "## Model 4: RF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = custom_train_test_split(data_processed_w_music, trainset_code, testset_code)\n",
    "\n",
    "imp = SimpleImputer(strategy='most_frequent') # 也可以选择'median'等其他填充策略\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', {0: 1, 1: 10}, {0: 1, 1: 5}]\n",
    "}\n",
    "# 3240 fits 44m\n",
    "\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "grid_search_rf = GridSearchCV(classifier, param_grid, scoring=scorer, cv=5, verbose=1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best AUC: {grid_search_rf.best_score_:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d02ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_rf, y_pred_rf = evaluate_best_model(grid_search_rf, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6159ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_default(y_pred_rf, y_test)\n",
    "\n",
    "plot_metrics_vs_threshold(y_test, y_pred_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab6756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.Series(best_model_rf.feature_importances_, index=X_columns)\n",
    "feature_importance_sorted = feature_importance.sort_values(ascending=False)[:10]\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance_sorted) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34212e3c",
   "metadata": {},
   "source": [
    "## Model 5: Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1829f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = custom_train_test_split(data_processed_w_music, trainset_code, testset_code)\n",
    "\n",
    "imp = SimpleImputer(strategy='most_frequent') # 也可以选择'median'等其他填充策略\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.transform(X_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', best_model_rf), \n",
    "    ('lr', best_model_lr), \n",
    "    ('xgb', best_model_xgb), \n",
    "    ('cat', best_model_cat)\n",
    "], voting='soft')  # use 'hard' for majority voting\n",
    "\n",
    "\n",
    "voting_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8253492",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "y_pred_prob_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report (Threshold = 0.5):\")\n",
    "print(classification_report(y_test, y_pred_voting))\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob_voting)\n",
    "\n",
    "print(f'Voting Classifier AUC: {roc_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb056017",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_default(y_pred_prob_voting, y_test)\n",
    "\n",
    "plot_metrics_vs_threshold(y_test, y_pred_prob_voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e71a41",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_classification(y_true, y_pred_proba):\n",
    "    y_pred = (np.array(y_pred_proba) > 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'AUC': auc,\n",
    "        'Accuracy': accuracy,\n",
    "        '0-Precision': precision[0],\n",
    "        '0-Recall': recall[0],\n",
    "        '0-F1-Score': f1[0],\n",
    "        '1-Precision': precision[1],\n",
    "        '1-Recall': recall[1],\n",
    "        '1-F1-Score': f1[1],\n",
    "        'Macro-Precision': precision_macro,\n",
    "        'Macro-Recall': recall_macro,\n",
    "        'Macro-F1-Score': f1_macro,\n",
    "        'Weighted-Precision': precision_weighted,\n",
    "        'Weighted-Recall': recall_weighted,\n",
    "        'Weighted-F1-Score': f1_weighted\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b545ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate models\n",
    "results_baseline = evaluate_classification(y_test, y_pred_baseline)\n",
    "\n",
    "results_lr = evaluate_classification(y_test, y_pred_lr)\n",
    "\n",
    "results_lr_expert = evaluate_classification(y_test, y_pred_lr_exprt)\n",
    "\n",
    "results_xgb = evaluate_classification(y_test, y_pred_xgb)\n",
    "\n",
    "results_cat = evaluate_classification(y_test, y_pred_cat)\n",
    "\n",
    "results_rf = evaluate_classification(y_test, y_pred_rf)\n",
    "\n",
    "results_voting = evaluate_classification(y_test, y_pred_prob_voting)\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Baseline': results_baseline,\n",
    "    'Logistic Regression': results_lr,\n",
    "    'LR(4 features)': results_lr_expert,\n",
    "    'XGBoost': results_xgb,\n",
    "    'CATBoost': results_cat,\n",
    "    'Random Forest': results_rf,\n",
    "    'Soft Voting': results_voting\n",
    "})\n",
    "\n",
    "# Transpose the DataFrame for better readability\n",
    "results_df = results_df.T\n",
    "\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_values(val):\n",
    "    val = 100*val\n",
    "    return f\"{val:.0f}%\"\n",
    "\n",
    "# Apply formatting\n",
    "styled_results_df = (results_df.style\n",
    "    .format(format_values)\n",
    "    .background_gradient(cmap='Greens')\n",
    "    .set_properties(**{'width': '120px'}))\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59b0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# # Define a custom color map from red to green\n",
    "# cmap = LinearSegmentedColormap.from_list(\"red_green\", [\"red\", \"white\", \"green\"])\n",
    "\n",
    "# # Normalize the feature values to a range between 0 and 1\n",
    "# norm = plt.Normalize(features.min().min(), features.max().max())\n",
    "\n",
    "# def apply_gradient(val):\n",
    "#     if abs(val) >= 0.001: \n",
    "#         color = cmap(norm(val))\n",
    "#         return 'background-color: rgba({}, {}, {}, {}); color: black'.format(int(color[0]*255), int(color[1]*255), int(color[2]*255), color[3])\n",
    "#     else: \n",
    "#         return 'color: white'\n",
    "# def highlight_negative(data, features):\n",
    "#     # Create a new DataFrame to hold the styles\n",
    "#     styles = pd.DataFrame('', index=data.index, columns=data.columns)\n",
    "    \n",
    "#     # Iterate over the DataFrame and apply the style conditionally\n",
    "#     for row in data.index:\n",
    "#         for col in data.columns:\n",
    "#             if col == 'MeanImportance': continue \n",
    "#             # if features.loc[row, col] < 0:\n",
    "#             #     styles.loc[row, col] = 'color: red'\n",
    "#             # elif features.loc[row, col] > 0:\n",
    "#             #     styles.loc[row, col] = 'color: green'\n",
    "#             value = features.loc[row, col]\n",
    "#             styles.loc[row, col] = apply_gradient(value)\n",
    "#     return styles\n",
    "\n",
    "# styled_df = feature_importance_df.sort_values('MeanImportance', ascending=False).style.apply(lambda x: highlight_negative(x, features), axis=None)\n",
    "\n",
    "\n",
    "# print(\"The values in the field show the feature importance. (Norm to 1 for each model) \")\n",
    "# print(\"The color in the background of the field show the negative(red) or positive(green) impact on average. \")\n",
    "# print(\"The darker it is, the higher the impact that feature has.\")\n",
    "# print(\"If the sum up between negative and positive shap values is too small (0.001), background color does not show. \")\n",
    "\n",
    "# # Display the styled DataFrame\n",
    "# styled_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
