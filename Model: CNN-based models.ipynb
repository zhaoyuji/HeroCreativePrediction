{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0938a654-e203-42b9-9ee6-c298e420bf82",
   "metadata": {},
   "source": [
    "This notebook is running on Amazon Sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae63ac-70a0-4e37-b087-fc70fd095abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, auc, classification_report, \n",
    "                             confusion_matrix, f1_score, make_scorer, \n",
    "                             precision_score, precision_recall_fscore_support, \n",
    "                             recall_score, roc_auc_score, roc_curve)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import (Concatenate, Dense, Dropout, \n",
    "                                     Flatten, GlobalAveragePooling2D, \n",
    "                                     GlobalMaxPooling2D, Input)\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "# Set environment variables for reproducibility\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5947ff6-c81e-4f20-b4b1-2f0b7962a439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbaaf8b-bdfa-4c9d-87ad-e504269ba264",
   "metadata": {},
   "source": [
    "## Data Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611ae8a-fb14-487d-bf6d-afb041ad10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset_train = pickle.loads(open('full_dataset_train.json', 'rb').read())\n",
    "dataset_test = pickle.loads(open('full_dataset_test.json', 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a95a28-5172-47dd-8c03-25588edb77d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset_train.keys())\n",
    "print(dataset_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515d9c5-e741-4f3f-863c-2398d792e1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_frame_features = dataset_train['train_frame_features']\n",
    "train_tag_features = dataset_train['train_tag_features']\n",
    "train_audio_features = dataset_train['train_audio_features']\n",
    "train_labels = dataset_train['train_labels']\n",
    "train_creative_code = dataset_train['train_creative_code']\n",
    "tag_column_name = dataset_train['tag_feature_name']\n",
    "\n",
    "test_frame_features = dataset_test['test_frame_features']\n",
    "test_tag_features = dataset_test['test_tag_features']\n",
    "test_audio_features = dataset_test['test_audio_features']\n",
    "test_labels = dataset_test['test_labels']\n",
    "test_creative_code = dataset_test['test_creative_code']\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"train_frame_features:\", train_frame_features.shape)\n",
    "print(\"train_tag_features:\", train_tag_features.shape)\n",
    "print(\"train_audio_features:\", train_audio_features.shape)\n",
    "print(\"train_labels:\", train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21e66b-e29f-4624-8f75-20c529d80a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TAG_N = train_tag_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ff033-b4c8-4472-92c3-a7a4efe850d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b932b2-6529-4356-b8e6-ffd58cd95774",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = ['feature_1', 'feature_2', 'feature_3', 'feature_4']\n",
    "\n",
    "indices = [tag_column_name.index(feature) for feature in baseline_features]\n",
    "\n",
    "print(indices)\n",
    "\n",
    "train_tag_base_features = tf.gather(train_tag_features, indices, axis=1)  \n",
    "test_tag_base_features = tf.gather(test_tag_features, indices, axis=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4fd0ac-86b7-47aa-aa3a-8e39f9754f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_mask = tf.math.is_nan(train_tag_base_features)\n",
    "nan_exists = tf.reduce_any(nan_mask)\n",
    "print(\"\\nDo any NaN values exist in the selected columns?\")\n",
    "print(nan_exists.numpy())\n",
    "\n",
    "nan_mask = tf.math.is_nan(test_tag_base_features)\n",
    "nan_exists = tf.reduce_any(nan_mask)\n",
    "print(\"\\nDo any NaN values exist in the selected columns?\")\n",
    "print(nan_exists.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8ce81-e7d9-4e1b-9885-b9a2930a6e4d",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930f1fe-b4cf-4497-b0a8-976824f1b248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels.numpy().reshape(-1)), y=train_labels.numpy().reshape(-1))\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "def evaluate_default(predictions, test_labels=test_labels, threshold=0.5):\n",
    "    predictions_binary = (predictions > threshold).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions_binary)\n",
    "    precision = precision_score(test_labels, predictions_binary)\n",
    "    recall = recall_score(test_labels, predictions_binary)\n",
    "    f1 = f1_score(test_labels, predictions_binary)\n",
    "    auc = roc_auc_score(test_labels, predictions)\n",
    "\n",
    "    print(f\"AUC: {auc}\")\n",
    "    \n",
    "     # Print classification report\n",
    "    print(\"\\n\"+\"*\"*30+\"Classification Report (Threshold = {threshold}):\".format(threshold=threshold))\n",
    "    print(classification_report(test_labels, predictions_binary))\n",
    "    print(confusion_matrix(test_labels, predictions_binary.reshape(-1)))\n",
    "\n",
    "def plot_metrics_vs_threshold(y_test, predictions):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "    \n",
    "    plot_ths = []\n",
    "    recall_at_thresholds = []\n",
    "    precision_at_thresholds = []\n",
    "    f1_at_thresholds = []\n",
    "    marco_recall_thresholds = []\n",
    "    best_marco_recall = -10e100\n",
    "    best_marco_recall_threshold = 0\n",
    "    # Calculate recall, precision, and F1 score at each threshold\n",
    "    for threshold in thresholds:\n",
    "        if threshold > 1: continue \n",
    "        y_pred_threshold = (predictions > threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_threshold, average=None)\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred_threshold, average='macro')\n",
    "        recall_at_thresholds.append(recall[1])\n",
    "        precision_at_thresholds.append(precision[1])\n",
    "        f1_at_thresholds.append(f1[1])\n",
    "        marco_recall_thresholds.append(recall_macro)\n",
    "        plot_ths.append(threshold)\n",
    "        # print(round(threshold,2), recall_macro)\n",
    "        if (recall_macro > best_marco_recall) and (recall[0] > 0.7) and (recall[1] > 0.3):\n",
    "            best_marco_recall = recall_macro\n",
    "            best_marco_recall_threshold = threshold\n",
    "    \n",
    "    print(\"\\n\"+\"*\"*30+\"Evaluate metrics vs. Threshold for true class + Marco Recall\")\n",
    "        \n",
    "    # Plot the metrics\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3), sharex=True)\n",
    "\n",
    "    # Plot recall vs. threshold\n",
    "    axs[0].plot(plot_ths, recall_at_thresholds, label='Recall', color='blue')\n",
    "    axs[0].set_xlabel('Threshold')\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[0].set_ylabel('Recall')\n",
    "    axs[0].set_title('Recall vs. Threshold')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot precision vs. threshold\n",
    "    axs[1].plot(plot_ths, precision_at_thresholds, label='Precision', color='green')\n",
    "    axs[1].set_xlabel('Threshold')\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[1].set_ylabel('Precision')\n",
    "    axs[1].set_title('Precision vs. Threshold')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Plot F1 score vs. threshold\n",
    "    axs[2].plot(plot_ths, f1_at_thresholds, label='F1 Score', color='red')\n",
    "    axs[2].set_xlabel('Threshold')\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[2].set_ylabel('F1 Score')\n",
    "    axs[2].set_title('F1 Score vs. Threshold')\n",
    "    axs[2].legend()\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    # Plot F1 score vs. threshold\n",
    "    axs[3].plot(plot_ths, marco_recall_thresholds, label='Marco Recall', color='black')\n",
    "    axs[3].set_xlabel('Threshold')\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[3].set_ylabel('Marco Recall')\n",
    "    axs[3].set_title('Marco Recall vs. Threshold')\n",
    "    axs[3].legend()\n",
    "    axs[3].grid(True)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print('best recall_macro', best_marco_recall)\n",
    "    print('best_marco_recall_threshold', best_marco_recall_threshold)\n",
    "    if best_marco_recall_threshold > 0:\n",
    "        evaluate_default(predictions, y_test, best_marco_recall_threshold)\n",
    "    else: \n",
    "        print('Cannot Beat Baseline.')\n",
    "\n",
    "def evaluate_classification(y_true, y_pred_proba):\n",
    "    y_pred = (np.array(y_pred_proba) > 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'AUC': auc,\n",
    "        'Accuracy': accuracy,\n",
    "        '0-Precision': precision[0],\n",
    "        '0-Recall': recall[0],\n",
    "        '0-F1-Score': f1[0],\n",
    "        '1-Precision': precision[1],\n",
    "        '1-Recall': recall[1],\n",
    "        '1-F1-Score': f1[1],\n",
    "        'Macro-Precision': precision_macro,\n",
    "        'Macro-Recall': recall_macro,\n",
    "        'Macro-F1-Score': f1_macro,\n",
    "        'Weighted-Precision': precision_weighted,\n",
    "        'Weighted-Recall': recall_weighted,\n",
    "        'Weighted-F1-Score': f1_weighted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210ac1d-290f-4dd3-8519-3c7b3e949265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def focal_loss_with_class_weights(gamma=2., alpha=0.25, class_weight=None):\n",
    "    ## Not used in current models - did nt perform well \n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_loss = -alpha_t * K.pow(1 - p_t, gamma) * K.log(p_t)\n",
    "        \n",
    "        if class_weight is not None:\n",
    "            weights = tf.reduce_sum(class_weight * y_true, axis=-1)\n",
    "            focal_loss *= weights\n",
    "        \n",
    "        return K.mean(focal_loss)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def train_model(model11, model_name, trainset, trainlabels, valset, vallabels, \n",
    "                epochs, batch_size,\n",
    "                if_callback, class_weights_setup):\n",
    "    checkpoint_filepath = 'model_checkpoints/{model_name}/best_model-{datetime}.h5'.format(model_name=model_name, datetime=datetime.today().strftime('%Y-%m-%d_%H:%M:%S'))\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss',  \n",
    "        mode='min',  \n",
    "        save_best_only=True\n",
    "    ) \n",
    "    \n",
    "    if if_callback:\n",
    "        callbacks_setup = [model_checkpoint_callback]\n",
    "    else: \n",
    "        callbacks_setup = None\n",
    "\n",
    "    history = model11.fit(trainset, \n",
    "                          trainlabels, \n",
    "                          epochs=epochs, \n",
    "                          batch_size=batch_size,\n",
    "                          class_weight=class_weights_setup,\n",
    "                          callbacks=callbacks_setup,\n",
    "                          validation_data=(valset, vallabels))\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['auc'], label='Training AUC')\n",
    "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation AUC')\n",
    "    plt.show()\n",
    " \n",
    "    return model11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c813f2-689d-4547-a3a2-f68414460eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL 1 One image + output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d7636-2551-481a-ba74-51719387a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_image = Input(shape=(224, 224, 3))\n",
    "\n",
    "base_model = resnet_base(input_image)\n",
    "x = Flatten()(base_model)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model11 = Model(inputs=input_image, outputs=output)\n",
    "\n",
    "model11.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[AUC(name='auc', curve='ROC')]\n",
    "              )\n",
    "model11.summary()\n",
    "\n",
    "# Total params: 75,100,033\n",
    "# Trainable params: 51,512,321\n",
    "# Non-trainable params: 23,587,712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66081789-4532-4ad3-ac0b-431b92eeeb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model11'\n",
    "trainset = train_frame_one_image\n",
    "trainlabels = train_labels\n",
    "valset = test_frame_one_image \n",
    "vallabels = test_labels\n",
    "class_weights_setup = class_weights_dict \n",
    "model11 = train_model(model11, model_name, trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, class_weights_setup=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84f957-8742-4618-8a03-24cb30cf4160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation = model11.evaluate(valset, vallabels)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "model11_predictions = model11.predict(valset)\n",
    "\n",
    "evaluate_default(model11_predictions)\n",
    "\n",
    "plot_metrics_vs_threshold(vallabels, model11_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd9620-6d0f-45bd-a27e-fce6f1bf49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a06f7-c26e-40ab-b326-971b125613b4",
   "metadata": {},
   "source": [
    "## MODEL 2: Pooling Way (Inspired by MVCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0b93f-3cdb-4c0a-8131-780b9f532446",
   "metadata": {},
   "source": [
    "### MODEL 2.1 total 6 images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9a4d3-e35f-4989-9d27-7b3d29115ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.004)\n",
    "\n",
    "pre_trained_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in pre_trained_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_frames = Input(shape=(6, 224, 224, 3))\n",
    "input_images = tf.unstack(input_frames, axis=1)\n",
    "\n",
    "view_features = [pre_trained_base(input_image) for input_image in input_images]\n",
    "view_pool = tf.reduce_max(tf.stack(view_features, axis=1), axis=1)\n",
    "\n",
    "flatten_view = Flatten()(view_pool)\n",
    "\n",
    "# # Fully connected layer (1)\n",
    "# fc6 = Dense(4096, activation='relu', kernel_regularizer=l2_reg, name='fc6')(flatten_view)\n",
    "# dropout6 = Dropout(0.6, name='dropout6')(fc6)\n",
    "\n",
    "# fc7 = Dense(4096, activation='relu', kernel_regularizer=l2_reg, name='fc7')(dropout6)\n",
    "# dropout7 = Dropout(0.6, name='dropout7')(fc7)\n",
    "\n",
    "# Fully connected layer (2)\n",
    "fc6 = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc6')(flatten_view)\n",
    "dropout6 = Dropout(0.6, name='dropout6')(fc6)\n",
    "\n",
    "fc7 = Dense(256, activation='relu', kernel_regularizer=l2_reg, name='fc7')(dropout6)\n",
    "dropout7 = Dropout(0.6, name='dropout7')(fc7)\n",
    "\n",
    "fc8 = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg, name='fc8')(dropout7)\n",
    "\n",
    "model21 = Model(inputs=input_frames, outputs=fc8)\n",
    "\n",
    "model21.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[AUC(name='auc', curve='ROC')])\n",
    "\n",
    "model21.summary()\n",
    "\n",
    "# resnet50 - original 4096 unit for fc6 fc7\n",
    "# Total params: 451,419,009\n",
    "# Trainable params: 427,831,297\n",
    "# Non-trainable params: 23,587,712\n",
    "\n",
    "# resnet50 - 512 + 256 - simpler network \n",
    "# Total params: 75,100,033\n",
    "# Trainable params: 51,512,321\n",
    "# Non-trainable params: 23,587,712\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d72a6-76b3-4bae-ab8e-a9c61a3c635a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ml.g4dn.4xlarge --> works for VGG16(7,7,512) with batch_size=32 but not Resnet50(7,7,2048) with batch_size even 4\n",
    "trainset = train_frame_features\n",
    "trainlabels = train_labels\n",
    "valset = test_frame_features \n",
    "vallabels = test_labels\n",
    "class_weights_setup = class_weights_dict \n",
    "model21 = train_model(model21, 'model21', trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, class_weights_setup=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377172a6-aac0-4482-9b77-641cfdd0dcb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation = model21.evaluate(valset, vallabels)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "model21_predictions = model21.predict(valset)\n",
    "evaluate_default(model21_predictions)\n",
    "plot_metrics_vs_threshold(vallabels, model21_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0824b-5c6c-4698-95fc-f1775faf1583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d6df898-8256-450b-8196-9290fd2ce0a3",
   "metadata": {},
   "source": [
    "### MODEL 2.2 total 6 images + audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550c5d9-6c70-4a9d-aa9e-22ecadc4ee7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.004)\n",
    "\n",
    "pre_trained_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in pre_trained_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_frames = Input(shape=(6, 224, 224, 3))\n",
    "input_images = tf.unstack(input_frames, axis=1)\n",
    "\n",
    "input_audio = Input(shape=(1024,), name='audio_input')  # Assuming YAMNet outputs 1024-dimensional embeddings\n",
    "\n",
    "view_features = [pre_trained_base(input_image) for input_image in input_images]\n",
    "view_pool = tf.reduce_max(tf.stack(view_features, axis=1), axis=1)\n",
    "\n",
    "flatten_view = Flatten()(view_pool)\n",
    "\n",
    "combined_features = Concatenate()([flatten_view, input_audio])\n",
    "\n",
    "fc6 = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc6')(combined_features)\n",
    "dropout6 = Dropout(0.6, name='dropout6')(fc6)\n",
    "\n",
    "fc7 = Dense(256, activation='relu', kernel_regularizer=l2_reg, name='fc7')(dropout6)\n",
    "dropout7 = Dropout(0.6, name='dropout7')(fc7)\n",
    "\n",
    "fc8 = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg, name='fc8')(dropout7)\n",
    "\n",
    "model22 = Model(inputs=[input_frames, input_audio], outputs=fc8)\n",
    "\n",
    "model22.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[AUC(name='auc', curve='ROC')])\n",
    "\n",
    "model22.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8da970-10bf-498d-af4f-e7cac9213c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainset = [train_frame_features, train_audio_features]\n",
    "trainlabels = train_labels\n",
    "valset = [test_frame_features, test_audio_features] \n",
    "vallabels = test_labels\n",
    "class_weights_setup = class_weights_dict \n",
    "model22 = train_model(model22, 'model22', trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, \n",
    "                      class_weights_setup=class_weights_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57539cbc-cf30-4660-8b7f-4053f36c76cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "evaluation = model22.evaluate(valset, vallabels)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "model22_predictions = model22.predict(valset)\n",
    "\n",
    "evaluate_default(model22_predictions)\n",
    "\n",
    "plot_metrics_vs_threshold(vallabels, model22_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d2c048-ff3b-437f-b4dd-0292be3a4f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd5197fc-c7f2-47f6-9144-1eb9741c31c7",
   "metadata": {},
   "source": [
    "### MODEL 2.3 total 6 images + audio + TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72db31-868d-4f55-ba91-c3ba41d395e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "l2_reg = keras.regularizers.l2(0.004)\n",
    "\n",
    "pre_trained_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in pre_trained_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_frames = Input(shape=(6, 224, 224, 3))\n",
    "input_images = tf.unstack(input_frames, axis=1)\n",
    "\n",
    "input_audio = Input(shape=(1024,), name='audio_input')  # Assuming YAMNet outputs 1024-dimensional embeddings\n",
    "input_tag = Input(shape=(TAG_N,), name='tag_input')  \n",
    "\n",
    "view_features = [pre_trained_base(input_image) for input_image in input_images]\n",
    "\n",
    "view_pool = tf.reduce_max(tf.stack(view_features, axis=1), axis=1)\n",
    "\n",
    "flatten_view = Flatten()(view_pool)\n",
    "\n",
    "\n",
    "combined_features = Concatenate()([flatten_view, input_audio, input_tag])\n",
    "\n",
    "# Fully connected layer (2)\n",
    "fc6 = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc6')(combined_features)\n",
    "dropout6 = Dropout(0.6, name='dropout6')(fc6)\n",
    "\n",
    "fc7 = Dense(256, activation='relu', kernel_regularizer=l2_reg, name='fc7')(dropout6)\n",
    "dropout7 = Dropout(0.6, name='dropout7')(fc7)\n",
    "\n",
    "fc8 = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg, name='fc8')(dropout7)\n",
    "\n",
    "model23 = Model(inputs=[input_frames, input_audio, input_tag], outputs=fc8)\n",
    "\n",
    "\n",
    "model23.compile(optimizer='adam', \n",
    "                loss='binary_crossentropy', \n",
    "              metrics=[AUC(name='auc', curve='ROC')])\n",
    "\n",
    "model23.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408037ab-8671-4d62-88c3-ccd2d643f139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "train_tag_features_imputed = imputer.fit_transform(train_tag_features)\n",
    "test_tag_features_imputed = imputer.transform(test_tag_features)\n",
    "\n",
    "\n",
    "trainset = [train_frame_features, train_audio_features, train_tag_features_imputed]\n",
    "trainlabels = train_labels\n",
    "valset = [test_frame_features, test_audio_features, test_tag_features_imputed] \n",
    "vallabels = test_labels\n",
    "class_weights_setup = class_weights_dict \n",
    "model23 = train_model(model23, 'model23', trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, class_weights_setup=class_weights_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c9c32-5327-459d-ac95-533b762c487a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "evaluation = model23.evaluate(valset, vallabels)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "model23_predictions = model23.predict(valset)\n",
    "\n",
    "evaluate_default(model23_predictions)\n",
    "\n",
    "plot_metrics_vs_threshold(vallabels, model23_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17439007-92be-4291-accf-9c71109b3390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eaad0fe-80ec-4942-bb2b-7c8c35feed57",
   "metadata": {},
   "source": [
    "### MODEL 2.4 6 images + audio + BASE TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34e9b7-6db1-4447-ab1c-0762cb032346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "l2_reg = keras.regularizers.l2(0.004)\n",
    "\n",
    "pre_trained_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in pre_trained_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_frames = Input(shape=(6, 224, 224, 3))\n",
    "input_images = tf.unstack(input_frames, axis=1)\n",
    "\n",
    "input_audio = Input(shape=(1024,), name='audio_input')  # Assuming YAMNet outputs 1024-dimensional embeddings\n",
    "input_tag = Input(shape=(4,), name='tag_input')  \n",
    "\n",
    "view_features = [pre_trained_base(input_image) for input_image in input_images]\n",
    "# Perform element-wise maximum to combine the view features\n",
    "view_pool = tf.reduce_max(tf.stack(view_features, axis=1), axis=1)\n",
    "\n",
    "flatten_view = Flatten()(view_pool)\n",
    "\n",
    "combined_features = Concatenate()([flatten_view, input_audio, input_tag])\n",
    "\n",
    "# Fully connected layer (2)\n",
    "fc6 = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc6')(combined_features)\n",
    "dropout6 = Dropout(0.6, name='dropout6')(fc6)\n",
    "\n",
    "fc7 = Dense(256, activation='relu', kernel_regularizer=l2_reg, name='fc7')(dropout6)\n",
    "dropout7 = Dropout(0.6, name='dropout7')(fc7)\n",
    "\n",
    "fc8 = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg, name='fc8')(dropout7)\n",
    "\n",
    "model232 = Model(inputs=[input_frames, input_audio, input_tag], outputs=fc8)\n",
    "\n",
    "model232.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[AUC(name='auc', curve='ROC')])\n",
    "\n",
    "model232.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50618c2d-3bfe-4e7f-937a-e9146a9bb7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainset = [train_frame_features, train_audio_features, train_tag_base_features]\n",
    "trainlabels = train_labels\n",
    "valset = [test_frame_features, test_audio_features, test_tag_base_features] \n",
    "vallabels = test_labels\n",
    "class_weights_setup = class_weights_dict \n",
    "model232 = train_model(model232, 'model232', trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, \n",
    "                     class_weights_setup=class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf78e74-a9ad-40dd-8082-7f5f7dc1cbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "evaluation = model232.evaluate(valset, vallabels)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "model232_predictions = model232.predict(valset)\n",
    "\n",
    "evaluate_default(model232_predictions)\n",
    "\n",
    "plot_metrics_vs_threshold(vallabels, model232_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ba7d2-3f7c-4496-8424-55c9f1680c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3e3899-6be0-4f7d-b9b4-c9dd5b55401d",
   "metadata": {},
   "source": [
    "## MODEL 3: Augment Way \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac11fd0-774a-4f56-9f36-fa1932331c7e",
   "metadata": {},
   "source": [
    "### Preprocess Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3380b6-8e96-4162-bb14-d0281d25c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_average(model, frame_features, audio_features, tag_features):\n",
    "    predictions = []\n",
    "    for i in range(frame_features.shape[1]):  # Iterate over the 6 images\n",
    "        image = frame_features[:, i, :, :, :]\n",
    "        audio = audio_features\n",
    "        tags = tag_features\n",
    "        pred = model.predict([image, audio, tags])\n",
    "        predictions.append(pred)\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    return avg_predictions.reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4322c4-7138-4c1c-b27a-d6895e7aafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_creative_code_expanded = [item for item in train_creative_code for _ in range(6)]\n",
    "\n",
    "train_frame_features_expanded = tf.reshape(train_frame_features, (-1, 224, 224, 3))\n",
    "\n",
    "train_tag_features_expanded = tf.tile(train_tag_features[:, tf.newaxis, :], [1, 6, 1])\n",
    "train_tag_features_expanded = tf.reshape(train_tag_features_expanded, (-1, 219))\n",
    "train_tag_base_features_expanded = tf.tile(train_tag_base_features[:, tf.newaxis, :], [1, 6, 1])\n",
    "train_tag_base_features_expanded = tf.reshape(train_tag_base_features_expanded, (-1, 4))\n",
    "\n",
    "train_audio_features_expanded = tf.tile(train_audio_features[:, tf.newaxis, :], [1, 6, 1])\n",
    "train_audio_features_expanded = tf.reshape(train_audio_features_expanded, (-1, 1024))\n",
    "\n",
    "train_labels_expanded = tf.tile(train_labels[:, tf.newaxis, :], [1, 6, 1])\n",
    "train_labels_expanded = tf.reshape(train_labels_expanded, (-1, 1))\n",
    "\n",
    "# Check the shapes\n",
    "print(\"Expanded train_frame_features shape:\", train_frame_features_expanded.shape)\n",
    "print(\"Expanded train_tag_features shape:\", train_tag_features_expanded.shape)\n",
    "print(\"Expanded train_tag_base_features shape:\", train_tag_base_features_expanded.shape)\n",
    "print(\"Expanded train_audio_features shape:\", train_audio_features_expanded.shape)\n",
    "print(\"Expanded train_labels shape:\", train_labels_expanded.shape)\n",
    "print(\"Expanded train_creative_code shape:\", len(train_creative_code_expanded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3406ee-d39f-4e9c-bb78-43c2ef5df4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_frame_features_expanded = tf.reshape(test_frame_features, (-1, 224, 224, 3))\n",
    "\n",
    "# Step 2: Duplicate the other features\n",
    "test_tag_features_expanded = tf.tile(test_tag_features[:, tf.newaxis, :], [1, 6, 1])\n",
    "test_tag_features_expanded = tf.reshape(test_tag_features_expanded, (-1, 219))\n",
    "\n",
    "test_tag_base_features_expanded = tf.tile(test_tag_base_features[:, tf.newaxis, :], [1, 6, 1])\n",
    "test_tag_base_features_expanded = tf.reshape(test_tag_base_features_expanded, (-1, 4))\n",
    "\n",
    "test_audio_features_expanded = tf.tile(test_audio_features[:, tf.newaxis, :], [1, 6, 1])\n",
    "test_audio_features_expanded = tf.reshape(test_audio_features_expanded, (-1, 1024))\n",
    "\n",
    "test_labels_expanded = tf.tile(test_labels[:, tf.newaxis, :], [1, 6, 1])\n",
    "test_labels_expanded = tf.reshape(test_labels_expanded, (-1, 1))\n",
    "\n",
    "test_creative_code_expanded = [item for item in test_creative_code for _ in range(6)]\n",
    "\n",
    "print(\"Expanded test_frame_features shape:\", test_frame_features_expanded.shape)\n",
    "print(\"Expanded test_tag_features shape:\", test_tag_features_expanded.shape)\n",
    "print(\"Expanded test_tag_base_features shape:\", test_tag_base_features_expanded.shape)\n",
    "print(\"Expanded test_audio_features shape:\", test_audio_features_expanded.shape)\n",
    "print(\"Expanded test_labels shape:\", test_labels_expanded.shape)\n",
    "print(\"Expanded test_creative_code shape:\", len(test_creative_code_expanded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7c5f5-3bf2-42e0-8fcc-26806ec03943",
   "metadata": {},
   "source": [
    "### MODEL 3.1 6 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7708110-d3bc-4722-b732-0243898a8907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "l2_reg = l2(0.004)\n",
    "\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_image = Input(shape=(224, 224, 3), name='image_input')\n",
    "image_features = resnet_base(input_image)\n",
    "flatten_image = GlobalAveragePooling2D()(image_features)\n",
    "\n",
    "fc1 = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc1')(flatten_image)\n",
    "dropout1 = Dropout(0.6, name='dropout1')(fc1)\n",
    "fc2 = Dense(256, activation='relu', kernel_regularizer=l2_reg, name='fc2')(dropout1)\n",
    "dropout2 = Dropout(0.6, name='dropout2')(fc2)\n",
    "output = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg, name='output')(dropout2)\n",
    "\n",
    "# Define the model\n",
    "model31 = Model(inputs=[input_image, input_audio, input_tag_base], outputs=output)\n",
    "model31.compile(optimizer='adam', \n",
    "                loss='binary_crossentropy', \n",
    "                metrics=[tf.keras.metrics.AUC(name='auc', curve='ROC')])\n",
    "\n",
    "model31.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b713a15-a877-4f82-bda9-08ba60269654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainset = [train_frame_features_expanded, train_audio_features_expanded, train_tag_base_features_expanded]\n",
    "trainlabels = train_labels_expanded\n",
    "valset = [test_frame_features_expanded, test_audio_features_expanded, test_tag_base_features_expanded] \n",
    "vallabels = test_labels_expanded\n",
    "class_weights_setup = class_weights_dict \n",
    "model31 = train_model(model31, 'model31', trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, class_weights_setup=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560a0d9-f503-4402-9f97-4d96977beeb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model31_predictions = predict_and_average(model31, test_frame_features, test_audio_features, test_tag_base_features)\n",
    "\n",
    "evaluate_default(model31_predictions)\n",
    "\n",
    "plot_metrics_vs_threshold(test_labels, model31_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff83ea-8dcf-4529-af9b-db4dc074f4fc",
   "metadata": {},
   "source": [
    "### MODEL 3.2 6 images + BASE TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e01b0e-cfcc-4275-8e7d-c6161804799e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "l2_reg = l2(0.004)\n",
    "\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_tag_base = Input(shape=(4,), name='input_tag_base_input')  \n",
    "\n",
    "input_image = Input(shape=(224, 224, 3), name='image_input')\n",
    "image_features = resnet_base(input_image)\n",
    "flatten_image = GlobalAveragePooling2D()(image_features)\n",
    "\n",
    "combined_features = Concatenate()([flatten_image, input_tag_base])\n",
    "\n",
    "fc1 = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc1')(combined_features)\n",
    "dropout1 = Dropout(0.6, name='dropout1')(fc1)\n",
    "fc2 = Dense(256, activation='relu', kernel_regularizer=l2_reg, name='fc2')(dropout1)\n",
    "dropout2 = Dropout(0.6, name='dropout2')(fc2)\n",
    "output = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg, name='output')(dropout2)\n",
    "\n",
    "# Define the model\n",
    "model32 = Model(inputs=[input_image, input_audio, input_tag_base], outputs=output)\n",
    "model32.compile(optimizer='adam',  \n",
    "                loss='binary_crossentropy', \n",
    "                metrics=[tf.keras.metrics.AUC(name='auc', curve='ROC')])\n",
    "\n",
    "model32.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3105e-c448-40a0-b386-5b2dac4f22d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainset = [train_frame_features_expanded, train_audio_features_expanded, train_tag_base_features_expanded]\n",
    "trainlabels = train_labels_expanded\n",
    "valset = [test_frame_features_expanded, test_audio_features_expanded, test_tag_base_features_expanded] \n",
    "vallabels = test_labels_expanded\n",
    "class_weights_setup = class_weights_dict \n",
    "model32 = train_model(model32, 'model32', trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, class_weights_setup=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302447b-aa88-4ccf-9ae3-a433e6697be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model32_predictions = predict_and_average(model32, test_frame_features, test_audio_features, test_tag_base_features)\n",
    "\n",
    "evaluate_default(model32_predictions)\n",
    "\n",
    "plot_metrics_vs_threshold(test_labels, model32_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808823c-9789-40ce-8355-f51f3ba59e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55d3ed80-7ded-457a-a010-3fa91422db65",
   "metadata": {},
   "source": [
    "### MODEL 3.3 6 images + audio + BASE TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630dd928-4bc5-46f3-852b-112642d42ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l2_reg = l2(0.004)\n",
    "\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_image = Input(shape=(224, 224, 3), name='image_input')\n",
    "input_audio = Input(shape=(1024,), name='audio_input')  # Assuming YAMNet outputs 1024-dimensional embeddings\n",
    "fc_audio = Dense(100, activation='relu', kernel_regularizer=l2_reg, name='fc_audio')(input_audio)\n",
    "input_tag_base = Input(shape=(4,), name='tag_base_input')  \n",
    "\n",
    "image_features = resnet_base(input_image)\n",
    "flatten_image = GlobalAveragePooling2D()(image_features)\n",
    "fc_image = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc_image')(flatten_image)\n",
    "\n",
    "combined_features = Concatenate()([fc_image, fc_audio, input_tag_base])\n",
    "\n",
    "fc1 = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc1')(combined_features)\n",
    "dropout1 = Dropout(0.6, name='dropout1')(fc1)\n",
    "fc2 = Dense(256, activation='relu', kernel_regularizer=l2_reg, name='fc2')(dropout1)\n",
    "dropout2 = Dropout(0.6, name='dropout2')(fc2)\n",
    "output = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg, name='output')(dropout2)\n",
    "\n",
    "model33 = Model(inputs=[input_image, input_audio, input_tag_base], outputs=output)\n",
    "model33.compile(optimizer='adam', \n",
    "                loss='binary_crossentropy', \n",
    "                metrics=[tf.keras.metrics.AUC(name='auc', curve='ROC')])\n",
    "\n",
    "model33.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab7db3-61a1-4e41-b381-cd57a461754d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainset = [train_frame_features_expanded, train_audio_features_expanded, train_tag_base_features_expanded]\n",
    "trainlabels = train_labels_expanded\n",
    "valset = [test_frame_features_expanded, test_audio_features_expanded, test_tag_base_features_expanded] \n",
    "vallabels = test_labels_expanded\n",
    "class_weights_setup = class_weights_dict \n",
    "model33 = train_model(model33, 'model33', trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, class_weights_setup=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899b4b5-9bf6-4a81-a323-24ff756d54af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model33_predictions = predict_and_average(model33, test_frame_features, test_audio_features, test_tag_base_features)\n",
    "\n",
    "evaluate_default(model33_predictions)\n",
    "\n",
    "plot_metrics_vs_threshold(test_labels, model33_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ff0c4-39fa-4fd6-90ae-2623bf3909bb",
   "metadata": {},
   "source": [
    "### MODEL 3.4 6 images + audio + ALL TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04286e98-2268-46e6-bb7f-f089bc3d9ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "l2_reg = l2(0.004)\n",
    "\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# Freeze the ResNet50 layers\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_image = Input(shape=(224, 224, 3), name='image_input')\n",
    "\n",
    "input_audio = Input(shape=(1024,), name='audio_input')  # Assuming YAMNet outputs 1024-dimensional embeddings\n",
    "fc_audio = Dense(100, activation='relu', kernel_regularizer=l2_reg, name='fc_audio')(input_audio)\n",
    "\n",
    "\n",
    "input_tag = Input(shape=(TAG_N,), name='input_tag')  \n",
    "\n",
    "image_features = resnet_base(input_image)\n",
    "flatten_image = GlobalAveragePooling2D()(image_features)\n",
    "fc_image = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc_image')(flatten_image)\n",
    "\n",
    "\n",
    "combined_features = Concatenate()([fc_image, fc_audio, input_tag])\n",
    "\n",
    "fc1 = Dense(512, activation='relu', kernel_regularizer=l2_reg, name='fc1')(combined_features)\n",
    "dropout1 = Dropout(0.6, name='dropout1')(fc1)\n",
    "fc2 = Dense(256, activation='relu', kernel_regularizer=l2_reg, name='fc2')(dropout1)\n",
    "dropout2 = Dropout(0.6, name='dropout2')(fc2)\n",
    "output = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg, name='output')(dropout2)\n",
    "\n",
    "model34 = Model(inputs=[input_image, input_audio, input_tag], outputs=output)\n",
    "model34.compile(optimizer='adam', \n",
    "                loss='binary_crossentropy', \n",
    "                metrics=[tf.keras.metrics.AUC(name='auc', curve='ROC')])\n",
    "\n",
    "model34.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a825c76-931e-480e-b55a-7c4b3c58c5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "train_tag_features_imputed = imputer.fit_transform(train_tag_features_expanded)\n",
    "test_tag_features_imputed = imputer.transform(test_tag_features_expanded)\n",
    "\n",
    "\n",
    "trainset = [train_frame_features_expanded, train_audio_features_expanded, train_tag_features_imputed]\n",
    "trainlabels = train_labels_expanded\n",
    "valset = [test_frame_features_expanded, test_audio_features_expanded, test_tag_features_imputed] \n",
    "vallabels = test_labels_expanded\n",
    "class_weights_setup = class_weights_dict \n",
    "model34 = train_model(model34, 'model34', trainset, trainlabels, valset, vallabels, \n",
    "                     epochs=20, batch_size=32,\n",
    "                     if_callback=True, class_weights_setup=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded4e01-9527-4308-a706-9a203fc69c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "test_tag_features_imputed = imputer.transform(test_tag_features)\n",
    "\n",
    "\n",
    "model34_predictions = predict_and_average(model34, test_frame_features, test_audio_features, test_tag_features_imputed)\n",
    "\n",
    "evaluate_default(model34_predictions)\n",
    "\n",
    "plot_metrics_vs_threshold(test_labels, model34_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8eb7de-c8f1-4445-a710-a0ad7005d7f5",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18539e-f186-4851-a975-ebe0431a0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results_11  = evaluate_classification(test_labels, model11_predictions)\n",
    "results_21  = evaluate_classification(test_labels, model21_predictions)\n",
    "results_22  = evaluate_classification(test_labels, model22_predictions)\n",
    "results_23  = evaluate_classification(test_labels, model23_predictions)\n",
    "results_232 = evaluate_classification(test_labels, model232_predictions)\n",
    "results_31  = evaluate_classification(test_labels, model31_predictions)\n",
    "results_32  = evaluate_classification(test_labels, model32_predictions)\n",
    "results_33  = evaluate_classification(test_labels, model33_predictions)\n",
    "results_34  = evaluate_classification(test_labels, model34_predictions)\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    '11: 1st Image (DNN Baseline)': results_11,\n",
    "    '21: 6 Images (Pooling)': results_21,\n",
    "    '22: 6 Images (Pooling) + Audio(1024d)': results_22,\n",
    "    '23: 6 Images (Pooling) + Audio(1024d) + TAG(219d)': results_23,\n",
    "    '232: 6 Images (Pooling) + Audio(1024d) + TAG(4d)': results_232,\n",
    "    '31: Augment: 1 image': results_31,\n",
    "    '32: Augment: 1 image TAG(4d)': results_32,\n",
    "    '33: Augment: 1 image + Audio(1024d) + TAG(4d)': results_33,\n",
    "    '34: Augment: 1 image + Audio(1024d) + TAG(219d)': results_34\n",
    "\n",
    "})\n",
    "\n",
    "# Transpose the DataFrame for better readability\n",
    "results_df = results_df.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f7d3a-a41f-408d-a1c0-5bdba23beb46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_values(val):\n",
    "    val = 100*val\n",
    "    return f\"{val:.0f}%\"\n",
    "\n",
    "# Apply formatting\n",
    "styled_results_df = (results_df.style\n",
    "    .format(format_values)\n",
    "    .background_gradient(cmap='Greens')\n",
    "    .set_properties(**{'width': '120px'}))\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591ea26-82d4-4bf8-8cae-03f3812fcfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ad1b323",
   "metadata": {},
   "source": [
    "## Best Model - Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84475eed-4d7c-44d8-b7c2-7051f7c671b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "final_model = load_model('model_checkpoints/final_model-2024-08-15_14:39:54.h5')\n",
    "\n",
    "model32_predictions = predict_and_average(final_model, test_frame_features, test_tag_base_features)\n",
    "\n",
    "evaluate_default(model32_predictions, test_labels)\n",
    "plot_metrics_vs_threshold(test_labels, model32_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae9cc7-85d9-4dcb-9273-0aeb5f4b9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_predictions = []\n",
    "for i in range(test_frame_features.shape[1]):  # Iterate over the 6 images\n",
    "    image = test_frame_features[:, i, :, :, :]\n",
    "    tags = test_tag_base_features\n",
    "    pred = final_model.predict([image, tags])\n",
    "    final_predictions.append(pred)\n",
    "\n",
    "df = pd.DataFrame(np.array(final_predictions).reshape(6, 246).T, columns=['1', '2', '3', '4', '5', '6'])\n",
    "\n",
    "means = df.median()\n",
    "# sorted_columns = means.sort_values().index\n",
    "# df_sorted = df[sorted_columns]\n",
    "df_melted = df.melt(var_name='Set', value_name='Values')\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(x='Set', y='Values', data=df_melted)\n",
    "for i, set_name in enumerate(['1', '2', '3', '4', '5', '6']):\n",
    "    mean_val = means[set_name]\n",
    "    ax.text(i, mean_val + 0.02, f'{mean_val:.2f}', ha='center', va='bottom', fontsize=10, color='black')\n",
    "plt.title('Box Plot of Predictions For Each Image')\n",
    "plt.xlabel('Images')\n",
    "plt.ylabel('Prediction')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.array(final_predictions).reshape(6, 246).T, columns=['1', '2', '3', '4', '5', '6'])\n",
    "df['target'] = test_labels\n",
    "df_melted = df.melt(var_name='Set', id_vars=['target'], value_name='Values')\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(x='Set', y='Values', data=df_melted, hue='target')\n",
    "plt.title('Box Plot of Predictions For Each Image By Real Target')\n",
    "plt.xlabel('Images')\n",
    "plt.ylabel('Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bf6f3-06f1-4dc4-8ff7-4658838aefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pos_creatives = set(np.array(test_creative_code)[np.where((model32_predictions>=0.5) == 1)[0]])\n",
    "real_pos_creatives = set(np.array(test_creative_code)[np.where(test_labels == 1)[0]])\n",
    "true_positive_samples = real_pos_creatives.intersection(pred_pos_creatives)\n",
    "print(true_positive_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea023fb-90bb-4f18-832b-a84ac9c4d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(test_labels == 1)[0]\n",
    "print(np.array(test_creative_code)[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdc24a-8982-46f0-8a20-4a23080852a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gradcam_heatmap(model, img_array, tag_array, last_conv_layer_name, pred_index=None):\n",
    "    with tf.device('/CPU:0'):\n",
    "        grad_model = Model(\n",
    "            [model.inputs], \n",
    "            [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = grad_model([img_array, tag_array])\n",
    "            if pred_index is None:\n",
    "                pred_index = tf.argmax(predictions[0])\n",
    "            class_channel = predictions[:, pred_index]\n",
    "\n",
    "        grads = tape.gradient(class_channel, conv_outputs)\n",
    "\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "        conv_outputs = conv_outputs[0]\n",
    "        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "        return heatmap.numpy()\n",
    "    \n",
    "def reverse_preprocess(img):\n",
    "    # Convert from BGR to RGB (since preprocess_input might convert it)\n",
    "    img = img[..., ::-1]\n",
    "    mean = np.array([103.939, 116.779, 123.68])\n",
    "    img = img + mean\n",
    "    img = np.clip(img, 0, 255)\n",
    "    return img \n",
    "\n",
    "def display_gradcam(img_array, heatmap, info, alpha=0.3):\n",
    "    img = img_array[0]\n",
    "    \n",
    "    img = reverse_preprocess(img)\n",
    "    img = img.astype('uint8')\n",
    "\n",
    "    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], (img.shape[0], img.shape[1]))\n",
    "    heatmap_resized = tf.squeeze(heatmap_resized, axis=-1)\n",
    "    heatmap_3d = tf.stack([heatmap_resized] * 3, axis=-1)    \n",
    "    heatmap_normalized = heatmap_3d / tf.reduce_max(heatmap_3d)\n",
    "    \n",
    "    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], (img.shape[0], img.shape[1]))\n",
    "    heatmap_resized = tf.squeeze(heatmap_resized, axis=-1)\n",
    "\n",
    "    heatmap2d_normalized = heatmap_resized / tf.reduce_max(heatmap_resized)\n",
    "\n",
    "    jet_heatmap = plt.cm.viridis(heatmap2d_normalized.numpy())[:, :, :3]  # Use 'plasma' or 'viridis'\n",
    "    img = tf.cast(img, dtype=tf.float32) / 255.0\n",
    "\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = np.uint8(255 * superimposed_img)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # # Heatmap\n",
    "    # plt.subplot(1, 4, 2)\n",
    "    # plt.imshow(heatmap_normalized, cmap='jet')\n",
    "    # plt.title('Heatmap')\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # Heatmap - REFINED \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(jet_heatmap)\n",
    "    plt.title('Refined Heatmap')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Superimposed image\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title('Superimposed Image')\n",
    "    plt.axis('off')\n",
    "    code = info['code']\n",
    "    no_img = info['no_img']\n",
    "    pred_score = info['pred_score']\n",
    "    fig.suptitle(f'Creative: {code} [No.{no_img}] - Model Pred = {pred_score:.2f}')      \n",
    "    fig.savefig(f'images_output/{code}_{no_img}.png')\n",
    "    plt.show()\n",
    "\n",
    "for creative_code in true_positive_samples:\n",
    "    print('#'*50, creative_code, '#'*50,)\n",
    "    hero_creative_index = np.where(np.array(test_creative_code_expanded) == creative_code)[0]\n",
    "\n",
    "    img_array = tf.gather(test_frame_features_expanded, hero_creative_index.tolist())\n",
    "    tag_array = tf.gather(test_tag_base_features_expanded, hero_creative_index.tolist()) \n",
    "    last_conv_layer_name = \"conv5_block3_out\"   \n",
    "\n",
    "    preds = final_model.predict([img_array,tag_array])\n",
    "\n",
    "    for i in range(6):\n",
    "        img = tf.expand_dims(img_array[i], axis=0)\n",
    "        tag = tf.expand_dims(tag_array[i], axis=0) \n",
    "        heatmap = get_gradcam_heatmap(final_model, img, tag, last_conv_layer_name)\n",
    "        display_gradcam(img, heatmap, info= {'code':creative_code, 'no_img': i, 'pred_score':preds[i][0]})\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.11.0 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.11.0-gpu-py39-cu112-ubuntu20.04-sagemaker-v1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
